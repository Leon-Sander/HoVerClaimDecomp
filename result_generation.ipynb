{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate_retrieval import is_successful_retrieval\n",
    "from utils import load_obj\n",
    "\n",
    "# tfidf_retrieval_output_dev_100.json\n",
    "# tfidf_retrieval_output_dev_1000.json\n",
    "# mistral_retrieval_output_dev_100.json\n",
    "# mistral_retrieval_output_dev_1000.json\n",
    "\n",
    "def calculate_success_percentage_supported(data, retrieval_key=\"retrieved\"):\n",
    "    supported_data = [item for item in data if item['label'] == \"SUPPORTED\"]\n",
    "    success_percentages, average_total_percentage = calculate_success_percentage(supported_data, retrieval_key)\n",
    "    return success_percentages, average_total_percentage\n",
    "\n",
    "def calculate_success_percentage_not_supported(data, retrieval_key=\"retrieved\"):\n",
    "    not_supported_data = [item for item in data if item['label'] == \"NOT_SUPPORTED\"]\n",
    "    success_percentages, average_total_percentage = calculate_success_percentage(not_supported_data, retrieval_key)\n",
    "    return success_percentages, average_total_percentage\n",
    "\n",
    "\n",
    "def calculate_success_percentage(data, retrieval_key=\"retrieved\"):\n",
    "    hop_counts = {2: {'total': 0, 'successful': 0},\n",
    "                    3: {'total': 0, 'successful': 0},\n",
    "                    4: {'total': 0, 'successful': 0}}\n",
    "\n",
    "    for obj in data:\n",
    "        num_hops = obj['num_hops']\n",
    "        hop_counts[num_hops][\"total\"] += 1\n",
    "        if is_successful_retrieval(obj, retrieval_key):\n",
    "            hop_counts[num_hops]['successful'] += 1\n",
    "\n",
    "    success_percentages = {}\n",
    "    for num_hops, counts in hop_counts.items():\n",
    "        if counts['total'] > 0:\n",
    "            success_percentage = (counts['successful'] / counts['total']) * 100\n",
    "            success_percentages[num_hops] = success_percentage\n",
    "    #print(hop_counts)\n",
    "    #print(success_percentages)\n",
    "    average_total_percentage = (success_percentages[2] + success_percentages[3] + success_percentages[4]) / 3\n",
    "    return success_percentages, average_total_percentage\n",
    "\n",
    "def calculate_success_percentage_qa(data, retrieval_key, support_type):\n",
    "    hop_counts = {2: {'total': 0, 'successful': 0},\n",
    "                    3: {'total': 0, 'successful': 0},\n",
    "                    4: {'total': 0, 'successful': 0}}\n",
    "\n",
    "\n",
    "    for hop_count in data:\n",
    "        int_hop_count = int(hop_count)\n",
    "        for key in data[hop_count]:\n",
    "            for item in data[hop_count][key]:\n",
    "                #if obj[\"label\"] == \"SUPPORTED\":\n",
    "                if support_type == \"SUPPORTED\":\n",
    "                    if item[\"label\"] == \"SUPPORTED\":\n",
    "                        hop_counts[int_hop_count][\"total\"] += 1\n",
    "                        if is_successful_retrieval(item, retrieval_key=retrieval_key):\n",
    "                            hop_counts[int_hop_count]['successful'] += 1\n",
    "                elif support_type == \"NOT_SUPPORTED\":\n",
    "                        if item[\"label\"] == \"NOT_SUPPORTED\":\n",
    "                            hop_counts[int_hop_count][\"total\"] += 1\n",
    "                            if is_successful_retrieval(item, retrieval_key=retrieval_key):\n",
    "                                hop_counts[int_hop_count]['successful'] += 1\n",
    "                else:\n",
    "                    hop_counts[int_hop_count][\"total\"] += 1\n",
    "                    if is_successful_retrieval(item, retrieval_key=retrieval_key):\n",
    "                        hop_counts[int_hop_count]['successful'] += 1\n",
    "    #print(hop_counts)\n",
    "    success_percentages = {}\n",
    "    for num_hops, counts in hop_counts.items():\n",
    "        if counts['total'] > 0:\n",
    "            success_percentage = (counts['successful'] / counts['total']) * 100\n",
    "            success_percentages[num_hops] = success_percentage\n",
    "\n",
    "\n",
    "    average_total_percentage = (success_percentages[2] + success_percentages[3] + success_percentages[4]) / 3\n",
    "    # Example variables\n",
    "    hops_2 = round(success_percentages[2],2)\n",
    "    hops_3 = round(success_percentages[3],2)\n",
    "    hops_4 = round(success_percentages[4],2)\n",
    "    avg_total = round(average_total_percentage,2)\n",
    "    return success_percentages, average_total_percentage#hops_2, hops_3, hops_4, avg_total\n",
    "\n",
    "\n",
    "def generate_latex_entry_simple(success_percentages, average_total_percentage, method, dataset, retrieved):\n",
    "    hops_2 = round(success_percentages[2],2)\n",
    "    hops_3 = round(success_percentages[3],2)\n",
    "    hops_4 = round(success_percentages[4],2)\n",
    "    avg_total = round(average_total_percentage,2)\n",
    "\n",
    "    #latex_line = f\"{method} & {dataset} & {retrieved} & {hops_2}\\% & {hops_3}\\% & {hops_4}\\% & {avg_total}\\% \\\\\\\\ \\\\hline\\n\"    \n",
    "    latex_line = f\"{method} & {hops_2}\\% & {hops_3}\\% & {hops_4}\\% & {avg_total}\\% \\\\\\\\ \\\\hline\\n\"   \n",
    "    print(latex_line)\n",
    "\n",
    "def generate_latex_entry_multicolumn(success_percentages_supp, average_total_percentage_supp, success_percentages_not_supp, average_total_percentage_not_supp, method):\n",
    "    hops_2_supp = round(success_percentages_supp[2],2)\n",
    "    hops_3_supp = round(success_percentages_supp[3],2)\n",
    "    hops_4_supp = round(success_percentages_supp[4],2)\n",
    "    avg_total_supp = round(average_total_percentage_supp,2)\n",
    "\n",
    "    hops_2_not_supp = round(success_percentages_not_supp[2],2)\n",
    "    hops_3_not_supp = round(success_percentages_not_supp[3],2)\n",
    "    hops_4_not_supp = round(success_percentages_not_supp[4],2)\n",
    "    avg_total_not_supp = round(average_total_percentage_not_supp,2)\n",
    "\n",
    "    #latex_line = f\"{method} & {dataset} & {retrieved} & {hops_2}\\% & {hops_3}\\% & {hops_4}\\% & {avg_total}\\% \\\\\\\\ \\\\hline\\n\"    \n",
    "    #latex_line = f\"{method} & {hops_2}\\% & {hops_3}\\% & {hops_4}\\% & {avg_total}\\% \\\\\\\\ \\\\hline\\n\"\n",
    "    # TF-IDF & 83.64\\% & 58.13\\% & 29.17\\% & 56.98\\% & 84.84\\% & 58.78\\% & 34.25\\% & 59.29\\% \\\\ \n",
    "    latex_line = f\"{method} & {hops_2_supp}\\% & {hops_3_supp}\\% & {hops_4_supp}\\% & {avg_total_supp}\\%& {hops_2_not_supp}\\% & {hops_3_not_supp}\\% & {hops_4_not_supp}\\% & {avg_total_not_supp}\\% \\\\\\\\\"\n",
    "    print(latex_line)\n",
    "\n",
    "def generate_latex_entry_multicolumn_with_total(success_percentages_supp, average_total_percentage_supp, success_percentages_not_supp, average_total_percentage_not_supp, total, method):\n",
    "    hops_2_supp = round(success_percentages_supp[2],2)\n",
    "    hops_3_supp = round(success_percentages_supp[3],2)\n",
    "    hops_4_supp = round(success_percentages_supp[4],2)\n",
    "    avg_total_supp = round(average_total_percentage_supp,2)\n",
    "\n",
    "    hops_2_not_supp = round(success_percentages_not_supp[2],2)\n",
    "    hops_3_not_supp = round(success_percentages_not_supp[3],2)\n",
    "    hops_4_not_supp = round(success_percentages_not_supp[4],2)\n",
    "    avg_total_not_supp = round(average_total_percentage_not_supp,2)\n",
    "\n",
    "    total = round(total,2)\n",
    "    #latex_line = f\"{method} & {dataset} & {retrieved} & {hops_2}\\% & {hops_3}\\% & {hops_4}\\% & {avg_total}\\% \\\\\\\\ \\\\hline\\n\"    \n",
    "    #latex_line = f\"{method} & {hops_2}\\% & {hops_3}\\% & {hops_4}\\% & {avg_total}\\% \\\\\\\\ \\\\hline\\n\"\n",
    "    # TF-IDF & 83.64\\% & 58.13\\% & 29.17\\% & 56.98\\% & 84.84\\% & 58.78\\% & 34.25\\% & 59.29\\% \\\\ \n",
    "    latex_line = f\"{method} & {hops_2_supp}\\% & {hops_3_supp}\\% & {hops_4_supp}\\% & {avg_total_supp}\\%& {hops_2_not_supp}\\% & {hops_3_not_supp}\\% & {hops_4_not_supp}\\% & {avg_total_not_supp}\\% & {total}\\%\\\\\\\\\"\n",
    "    print(latex_line)\n",
    "# initial Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF & 69.67\\% & 35.43\\% & 14.09\\% & 39.73\\%& 70.91\\% & 34.14\\% & 12.5\\% & 39.18\\% \\\\\n",
      "Mistral & 74.47\\% & 39.46\\% & 16.44\\% & 43.46\\%& 66.12\\% & 34.95\\% & 17.05\\% & 39.37\\% \\\\\n"
     ]
    }
   ],
   "source": [
    "data_path = \"data/tfidf_retrieval_output_dev_100.json\"\n",
    "success_percentages_supp, average_total_percentage_supp = calculate_success_percentage_supported(load_obj(data_path))\n",
    "success_percentages_not_supp, average_total_percentage_not_supp = calculate_success_percentage_not_supported(load_obj(data_path))\n",
    "generate_latex_entry_multicolumn(success_percentages_supp, average_total_percentage_supp, success_percentages_not_supp, average_total_percentage_not_supp, \"TF-IDF\")\n",
    "\n",
    "data_path = \"data/mistral_retrieval_output_dev_100.json\"\n",
    "success_percentages_supp, average_total_percentage_supp = calculate_success_percentage_supported(load_obj(data_path))\n",
    "success_percentages_not_supp, average_total_percentage_not_supp = calculate_success_percentage_not_supported(load_obj(data_path))\n",
    "generate_latex_entry_multicolumn(success_percentages_supp, average_total_percentage_supp, success_percentages_not_supp, average_total_percentage_not_supp, \"Mistral\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mistral & 71.21\\% & 37.6\\% & 15.46\\% & 41.42\\%& 63.31\\% & 32.76\\% & 15.15\\% & 37.07\\% \\\\\n"
     ]
    }
   ],
   "source": [
    "data_path = \"data/mistral_retrieval_output_dev_100_multi_hop_prompt.json\"\n",
    "success_percentages_supp, average_total_percentage_supp = calculate_success_percentage_supported(load_obj(data_path))\n",
    "success_percentages_not_supp, average_total_percentage_not_supp = calculate_success_percentage_not_supported(load_obj(data_path))\n",
    "generate_latex_entry_multicolumn(success_percentages_supp, average_total_percentage_supp, success_percentages_not_supp, average_total_percentage_not_supp, \"Mistral\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF & 84.84\\% & 58.78\\% & 34.25\\% & 59.29\\%& 83.64\\% & 58.13\\% & 29.17\\% & 56.98\\% \\\\\n",
      "Mistral & 92.71\\% & 69.21\\% & 45.6\\% & 69.17\\%& 87.77\\% & 63.67\\% & 44.7\\% & 65.38\\% \\\\\n"
     ]
    }
   ],
   "source": [
    "data_path = \"data/tfidf_retrieval_output_dev_1000.json\"\n",
    "success_percentages_supp, average_total_percentage_supp = calculate_success_percentage_supported(load_obj(data_path))\n",
    "success_percentages_not_supp, average_total_percentage_not_supp = calculate_success_percentage_not_supported(load_obj(data_path))\n",
    "generate_latex_entry_multicolumn(success_percentages_supp, average_total_percentage_supp, success_percentages_not_supp, average_total_percentage_not_supp, \"TF-IDF\")\n",
    "\n",
    "\n",
    "data_path = \"data/mistral_retrieval_output_dev_1000.json\"\n",
    "success_percentages_supp, average_total_percentage_supp = calculate_success_percentage_supported(load_obj(data_path))\n",
    "success_percentages_not_supp, average_total_percentage_not_supp = calculate_success_percentage_not_supported(load_obj(data_path))\n",
    "generate_latex_entry_multicolumn(success_percentages_supp, average_total_percentage_supp, success_percentages_not_supp, average_total_percentage_not_supp, \"Mistral\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = load_obj(\"data/decomp_baseline_FULL_DATASET.json\")\n",
    "#data = load_obj('/home/sander/code/thesis/hover/leon/data/decomp_TFIDF_baseline.json')\n",
    "data = load_obj(\"data/decomp_baseline_FULL_DATASET_9shot_refined.json\")\n",
    "from utils import save_obj\n",
    "run_count = 0\n",
    "## BASELINE\n",
    "for hop_count in data:\n",
    "    for key in data[hop_count]:\n",
    "        for item in data[hop_count][key]:\n",
    "\n",
    "            decomp_retrieval_100 = []\n",
    "            for index in range(100):\n",
    "                for retrieval in item[f\"decomposed_claims_retrieval_{run_count}\"]:\n",
    "                    #\n",
    "                    if len(decomp_retrieval_100) >= 100:\n",
    "                        break\n",
    "                    if index < len(retrieval):\n",
    "                        decomp_retrieval_100.append(retrieval[index])\n",
    "            item[f\"decomposed_claims_retrieval_100_mistral_no_filter\"] = decomp_retrieval_100\n",
    "save_obj(data, \"data/decomp_baseline_FULL_DATASET_9shot_refined.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:green;\">Qualitative Analysis Data</h1>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 & 72.55\\% & 38.43\\% & 18.98\\% & 43.32\\%& 61.82\\% & 34.26\\% & 16.1\\% & 37.39\\% \\\\\n",
      "10 & 72.17\\% & 39.26\\% & 18.98\\% & 43.47\\%& 64.96\\% & 37.37\\% & 16.48\\% & 39.6\\% \\\\\n",
      "20 & 71.79\\% & 41.12\\% & 19.77\\% & 44.22\\%& 63.64\\% & 36.22\\% & 17.23\\% & 39.03\\% \\\\\n",
      "60 & 73.51\\% & 39.46\\% & 19.37\\% & 44.12\\%& 67.6\\% & 34.83\\% & 17.42\\% & 39.95\\% \\\\\n"
     ]
    }
   ],
   "source": [
    "for threshhold in [5, 10, 20, 60]: #, 80, 125, 300\n",
    "    #data_path = f\"data/iterative_cross_test_{threshhold}_FULL_DATA.json\"\n",
    "    data_path = f\"data/iterative_cross_test_{threshhold}_FULL_DATA_TFIDF.json\"\n",
    "    success_percentages_supp, average_total_percentage_supp = calculate_success_percentage_qa(load_obj(data_path), retrieval_key=\"tfidf_retrieved_1\", support_type=\"SUPPORTED\")\n",
    "    success_percentages_not_supp, average_total_percentage_not_supp = calculate_success_percentage_qa(load_obj(data_path), retrieval_key=\"tfidf_retrieved_1\", support_type=\"NOT_SUPPORTED\")\n",
    "    generate_latex_entry_multicolumn(success_percentages_supp, average_total_percentage_supp, success_percentages_not_supp, average_total_percentage_not_supp, threshhold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 & 66.79\\% & 36.46\\% & 17.52\\% & 40.25\\% \\\\\n",
      "10 & 68.29\\% & 38.37\\% & 17.71\\% & 41.46\\% \\\\\n",
      "20 & 67.41\\% & 38.8\\% & 18.48\\% & 41.56\\% \\\\\n",
      "60 & 70.34\\% & 37.28\\% & 18.38\\% & 42.0\\% \\\\\n"
     ]
    }
   ],
   "source": [
    "def generate_latex_entry_multicolumn_all(success_percentages, average_total_percentage, method):\n",
    "    hops_2_supp = round(success_percentages[2],2)\n",
    "    hops_3_supp = round(success_percentages[3],2)\n",
    "    hops_4_supp = round(success_percentages[4],2)\n",
    "    avg_total_supp = round(average_total_percentage,2)\n",
    "\n",
    "    #latex_line = f\"{method} & {dataset} & {retrieved} & {hops_2}\\% & {hops_3}\\% & {hops_4}\\% & {avg_total}\\% \\\\\\\\ \\\\hline\\n\"    \n",
    "    #latex_line = f\"{method} & {hops_2}\\% & {hops_3}\\% & {hops_4}\\% & {avg_total}\\% \\\\\\\\ \\\\hline\\n\"\n",
    "    # TF-IDF & 83.64\\% & 58.13\\% & 29.17\\% & 56.98\\% & 84.84\\% & 58.78\\% & 34.25\\% & 59.29\\% \\\\ \n",
    "    latex_line = f\"{method} & {hops_2_supp}\\% & {hops_3_supp}\\% & {hops_4_supp}\\% & {avg_total_supp}\\% \\\\\\\\\"\n",
    "    print(latex_line)\n",
    "\n",
    "for threshhold in [5, 10, 20, 60]: #, 80, 125, 300\n",
    "    #data_path = f\"data/iterative_cross_test_{threshhold}_FULL_DATA.json\"\n",
    "    data_path = f\"data/iterative_cross_test_{threshhold}_FULL_DATA_TFIDF.json\"\n",
    "    success_percentages_supp, average_total_percentage_supp = calculate_success_percentage_qa(load_obj(data_path), retrieval_key=\"tfidf_retrieved_1\", support_type=\"ALL\")\n",
    "    generate_latex_entry_multicolumn_all(success_percentages_supp, average_total_percentage_supp, threshhold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 & 69.67\\% & 39.46\\% & 17.61\\% & 42.25\\%& 68.43\\% & 36.91\\% & 14.39\\% & 39.91\\% & 41.08\\%\\\\\n",
      "10 & 72.36\\% & 40.19\\% & 18.59\\% & 43.71\\%& 71.07\\% & 40.6\\% & 17.99\\% & 43.22\\% & 43.45\\%\\\\\n",
      "20 & 72.55\\% & 41.12\\% & 20.16\\% & 44.61\\%& 71.24\\% & 41.64\\% & 18.18\\% & 43.69\\% & 44.12\\%\\\\\n",
      "60 & 72.74\\% & 41.01\\% & 20.94\\% & 44.9\\%& 71.24\\% & 39.33\\% & 18.56\\% & 43.04\\% & 43.96\\%\\\\\n"
     ]
    }
   ],
   "source": [
    "for threshhold in [5, 10, 20, 60]: #, 80, 125, 300\n",
    "    #data_path = f\"data/iterative_cross_test_{threshhold}_FULL_DATA.json\"\n",
    "    data_path = f\"data/iterative_cross_test_{threshhold}_FULL_DATA_TFIDF.json\"\n",
    "    success_percentages_supp, average_total_percentage_supp = calculate_success_percentage_qa(load_obj(data_path), retrieval_key=\"tfidf_retrieved_1\", support_type=\"SUPPORTED\")\n",
    "    success_percentages_not_supp, average_total_percentage_not_supp = calculate_success_percentage_qa(load_obj(data_path), retrieval_key=\"tfidf_retrieved_1\", support_type=\"NOT_SUPPORTED\")\n",
    "    _, total = calculate_success_percentage_qa(load_obj(data_path), retrieval_key=\"tfidf_retrieved_1\", support_type=\"ALL\")\n",
    "    generate_latex_entry_multicolumn_with_total(success_percentages_supp, average_total_percentage_supp, success_percentages_not_supp, average_total_percentage_not_supp, total,threshhold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = load_obj(\"data/decomp_baseline_FULL_DATASET.json\")\n",
    "#data = load_obj('/home/sander/code/thesis/hover/leon/data/decomp_TFIDF_baseline.json')\n",
    "from utils import save_obj, load_obj\n",
    "data = load_obj(\"data/decomp_baseline_FULL_DATASET_9shot_NOINSTRUCT.json\")\n",
    "\n",
    "run_count = 0\n",
    "## BASELINE\n",
    "for hop_count in data:\n",
    "    for key in data[hop_count]:\n",
    "        for item in data[hop_count][key]:\n",
    "\n",
    "            decomp_retrieval_100 = []\n",
    "            for index in range(100):\n",
    "                for retrieval in item[f\"decomposed_claims_retrieval_{run_count}\"]:\n",
    "                #for retrieval in item[f\"decomposed_claims_tfidf_retrieved\"]:\n",
    "                    \n",
    "                    #\n",
    "                    if len(decomp_retrieval_100) >= 100:\n",
    "                        break\n",
    "                    if index < len(retrieval):\n",
    "                        decomp_retrieval_100.append(retrieval[index])\n",
    "            #item[f\"decomposed_claims_retrieval_100_tfidf_no_filter\"] = decomp_retrieval_100\n",
    "            item[f\"decomposed_claims_retrieval_100_mistral_no_filter\"] = decomp_retrieval_100\n",
    "save_obj(data, \"data/decomp_baseline_FULL_DATASET_9shot_NOINSTRUCT.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = load_obj(\"data/decomp_baseline_FULL_DATASET.json\")\n",
    "#data = load_obj('/home/sander/code/thesis/hover/leon/data/decomp_TFIDF_baseline.json')\n",
    "data = load_obj(\"/home/sander/code/thesis/hover/leon/data/decomp_baseline_FULL_DATASET_FINAL_tfidf.json\")\n",
    "\n",
    "from utils import save_obj\n",
    "run_count = 0\n",
    "## BASELINE with filter\n",
    "for hop_count in data:\n",
    "    for key in data[hop_count]:\n",
    "        for item in data[hop_count][key]:\n",
    "\n",
    "            decomp_retrieval_100 = []\n",
    "            for index in range(100):\n",
    "                #for retrieval in item[f\"decomposed_claims_retrieval_{run_count}\"]:\n",
    "                for retrieval in item[f\"decomposed_claims_tfidf_retrieved\"]:\n",
    "                    \n",
    "                    #\n",
    "                    if len(decomp_retrieval_100) >= 100:\n",
    "                        break\n",
    "                    if index < len(retrieval):\n",
    "                        if retrieval[index] not in decomp_retrieval_100:\n",
    "                            decomp_retrieval_100.append(retrieval[index])\n",
    "\n",
    "            item[f\"decomposed_claims_retrieval_100_tfidf_filter\"] = decomp_retrieval_100\n",
    "            #item[f\"decomposed_claims_retrieval_100_mistral_filter\"] = decomp_retrieval_100\n",
    "save_obj(data, \"/home/sander/code/thesis/hover/leon/data/decomp_baseline_FULL_DATASET_FINAL_tfidf.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import save_obj, load_obj\n",
    "#data = load_obj(\"/home/sander/code/thesis/hover/leon/data/iterative_FULL_DATASET_with_questions_NEW2_TFIDF.json\")\n",
    "####\n",
    "#####ITERATIV\n",
    "#####\n",
    "\n",
    "data = load_obj(\"data/iterative_decomp_FULL_DATASET_1.json\")\n",
    "for hop_count in data:\n",
    "    for key in data[hop_count]:\n",
    "        for item in data[hop_count][key]:\n",
    "\n",
    "\n",
    "            for run_count in range(5):\n",
    "                if run_count <= int(hop_count):\n",
    "\n",
    "\n",
    "                    decomp_retrieval_100 = []\n",
    "                    for index in range(100):\n",
    "                        for retrieval in item[f\"decomposed_claims_retrieval_{run_count}\"]:\n",
    "                            #\n",
    "                            if len(decomp_retrieval_100) >= 100:\n",
    "                                break\n",
    "                            if index < len(retrieval):\n",
    "                                decomp_retrieval_100.append(retrieval[index])\n",
    "                    item[f\"decomposed_claims_retrieval_100_combined_{run_count}\"] = decomp_retrieval_100\n",
    "save_obj(data, \"data/iterative_decomp_FULL_DATASET_1.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import save_obj, load_obj\n",
    "#data = load_obj(\"/home/sander/code/thesis/hover/leon/data/iterative_FULL_DATASET_with_questions_NEW2_TFIDF.json\")\n",
    "####\n",
    "#####ITERATIV COMBINATION OF DECOMPOSED RETRIEVALS\n",
    "#####\n",
    "\n",
    "data = load_obj(\"data/iterative_decomp_FULL_DATASET_1.json\")\n",
    "for hop_count in data:\n",
    "    for key in data[hop_count]:\n",
    "        for item in data[hop_count][key]:\n",
    "            for run_count in range(5):\n",
    "                if run_count <= int(hop_count):\n",
    "                    decomp_combined_retrieval = []\n",
    "\n",
    "                    for retrieval in item[f\"decomposed_claims_retrieval_{run_count}\"]:\n",
    "                        decomp_combined_retrieval.extend(retrieval)\n",
    "                    item[f\"decomposed_combined_{run_count}\"] = decomp_combined_retrieval\n",
    "save_obj(data, \"data/iterative_decomp_FULL_DATASET_1_combined.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Instruction & 77.93\\% & 43.29\\% & 21.92\\% & 47.71\\%& 72.56\\% & 38.75\\% & 20.08\\% & 43.8\\% \\\\\n"
     ]
    }
   ],
   "source": [
    "#data_path= \"/home/sander/code/thesis/hover/leon/data/decomp_baseline_FULL_DATASET_FINAL_tfidf.json\"\n",
    "#data_path =\"/home/sander/code/thesis/hover/leon/data/decomp_baseline_FULL_DATASET_FINAL.json\"\n",
    "data_path = \"data/decomp_baseline_FULL_DATASET_9shot_NOINSTRUCT.json\"\n",
    "success_percentages_supp, average_total_percentage_supp = calculate_success_percentage_qa(load_obj(data_path), retrieval_key=\"decomposed_claims_retrieval_100_mistral_no_filter\", support_type=\"SUPPORTED\")\n",
    "success_percentages_not_supp, average_total_percentage_not_supp = calculate_success_percentage_qa(load_obj(data_path), retrieval_key=\"decomposed_claims_retrieval_100_mistral_no_filter\", support_type=\"NOT_SUPPORTED\")\n",
    "generate_latex_entry_multicolumn(success_percentages_supp, average_total_percentage_supp, success_percentages_not_supp, average_total_percentage_not_supp, \"No Instruction\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\multicolumn{6}{l}{\\textit{Hop 2, SUPPORTED}} \\\\\n",
      "decomp 1 & 80.0\\% & 82.7\\% & 80.0\\% & - & - \\\\\n",
      "\\multicolumn{6}{l}{\\textit{Hop 2, NOT_SUPPORTED}} \\\\\n",
      "decomp 1 & 76.2\\% & 75.4\\% & 72.7\\% & - & - \\\\\n",
      "\\multicolumn{6}{l}{\\textit{Hop 3, SUPPORTED}} \\\\\n",
      "decomp 1 & 48.2\\% & 53.4\\% & 51.1\\% & 50.5\\% & - \\\\\n",
      "\\multicolumn{6}{l}{\\textit{Hop 3, NOT_SUPPORTED}} \\\\\n",
      "decomp 1 & 43.9\\% & 46.9\\% & 45.1\\% & 43.6\\% & - \\\\\n",
      "\\multicolumn{6}{l}{\\textit{Hop 4, SUPPORTED}} \\\\\n",
      "decomp 1 & 27.4\\% & 36.6\\% & 36.8\\% & 34.6\\% & 33.7\\% \\\\\n",
      "\\multicolumn{6}{l}{\\textit{Hop 4, NOT_SUPPORTED}} \\\\\n",
      "decomp 1 & 23.7\\% & 26.7\\% & 25.2\\% & 23.3\\% & 23.3\\% \\\\\n"
     ]
    }
   ],
   "source": [
    "def generate_latex_rows_for_method_qa(data, max_iter, method, retrieval_key):\n",
    "    # Print rows for each hop count for the specific method\n",
    "    for hop_count in data:\n",
    "        for key in data[hop_count]:\n",
    "        #hop_count = int(hop_count)\n",
    "        # Supported\n",
    "            row_entries_supported = []\n",
    "            print(f\"\\\\multicolumn{{6}}{{l}}{{\\\\textit{{Hop {hop_count}, {key}}}}} \\\\\\\\\")\n",
    "            for i in range(max_iter + 1):\n",
    "                if i <= int(hop_count):\n",
    "                    success = 0\n",
    "                    for item_index, item in enumerate(data[hop_count][key]):\n",
    "                        try:\n",
    "                            success += is_successful_retrieval(item, f\"{retrieval_key}_{i}\")\n",
    "                        except:\n",
    "                            print(f\"error for item: {hop_count}, {key}, {i}, {item_index}\")\n",
    "\n",
    "\n",
    "                    total = sum(1 for item in data[hop_count][key])\n",
    "                    success_rate = success / total * 100 if total > 0 else 0\n",
    "                    row_entries_supported.append(f\"{success_rate:.1f}\\%\")\n",
    "                else:\n",
    "                    row_entries_supported.append(\"-\")\n",
    "            print(method + \" & \" + \" & \".join(row_entries_supported) + \" \\\\\\\\\")\n",
    "\n",
    "#data = load_obj(\"data/iterative_FULL_DATASET_with_questions_60_no_filter.json\")\n",
    "data = load_obj(\"data/iterative_decomp_FULL_DATASET_1_combined.json\")\n",
    "#data = load_obj(\"data/iterative_decomp_FULL_DATASET_2.json\")\n",
    "generate_latex_rows_for_method_qa(data, 4, \"decomp 1\", \"decomposed_combined\")\n",
    "\n",
    "## nochmal 체berpr체fen obs hier auch mit rechten Dingen zugeht."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "textbf{Iter 0} & textbf{Iter 1} & textbf{Iter 2} & textbf{Iter 3} & textbf{Iter 4}\n",
      "tfidf & 41.17\\% & 41.40\\% & 39.45\\% & 38.67\\% & 38.50\\%\\\\\n"
     ]
    }
   ],
   "source": [
    "def total_retrieval_success_across_iterations(data, max_iter):\n",
    "    success_dict ={0 : {\"total\" : 0, \"successful\" : 0},\n",
    "                   1 : {\"total\" : 0, \"successful\" : 0},\n",
    "                   2 : {\"total\" : 0, \"successful\" : 0},\n",
    "                   3 : {\"total\" : 0, \"successful\" : 0},\n",
    "                   4 : {\"total\" : 0, \"successful\" : 0}}\n",
    "    for run_count in range(max_iter + 1):\n",
    "        for hop_count in data:\n",
    "            #if run_count <= int(hop_count):\n",
    "                for key in data[hop_count]:\n",
    "                    for item_index, item in enumerate(data[hop_count][key]):\n",
    "                        success_dict[run_count][\"total\"] += 1\n",
    "                        try:\n",
    "                            if run_count > 2 and hop_count == \"2\":\n",
    "                                if is_successful_retrieval(item, f\"retrieved_{2}\"):\n",
    "                                    success_dict[run_count][\"successful\"] += 1\n",
    "                            elif run_count > 3 and hop_count == \"3\":\n",
    "                                if is_successful_retrieval(item, f\"retrieved_{3}\"):\n",
    "                                    success_dict[run_count][\"successful\"] += 1\n",
    "\n",
    "                            elif is_successful_retrieval(item, f\"retrieved_{run_count}\"):\n",
    "                                success_dict[run_count][\"successful\"] += 1\n",
    "                        except:\n",
    "                            print(f\"Error for item {hop_count}, {key}, index {item_index}\")\n",
    "\n",
    "    print(\"textbf{Iter 0} & textbf{Iter 1} & textbf{Iter 2} & textbf{Iter 3} & textbf{Iter 4}\")\n",
    "    #print(f\"{success_dict[0]['successful']/success_dict[0]['total']} & {success_dict[1]['successful']/success_dict[1]['total']} & {success_dict[2]['successful']/success_dict[2]['total']} & {success_dict[3]['successful']/success_dict[3]['total']} & {success_dict[4]['successful']/success_dict[4]['total']}\")\n",
    "    print(f\"tfidf & {success_dict[0]['successful']/success_dict[0]['total'] * 100:.2f}\\% & {success_dict[1]['successful']/success_dict[1]['total'] * 100:.2f}\\% & {success_dict[2]['successful']/success_dict[2]['total'] * 100:.2f}\\% & {success_dict[3]['successful']/success_dict[3]['total'] * 100:.2f}\\% & {success_dict[4]['successful']/success_dict[4]['total'] * 100:.2f}\\%\\\\\\\\\")\n",
    "\n",
    "from utils import load_obj\n",
    "data = load_obj(\"data/iterative_FULL_DATASET_with_questions_NEW2.json\")\n",
    "total_retrieval_success_across_iterations(data, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n",
      "4764\n",
      "2.382\n"
     ]
    }
   ],
   "source": [
    "from utils import load_obj, save_obj\n",
    "data = load_obj(\"data/iterative_FULL_DATASET_with_questions_NEW2.json\")\n",
    "new_data = {}\n",
    "total = 0\n",
    "subquestions = 0\n",
    "for hop_count in data:\n",
    "    new_data[hop_count] = {\"SUPPORTED\" : []}\n",
    "    for item in data[hop_count][\"SUPPORTED\"]:\n",
    "        total += 1\n",
    "        subquestions += len(item[\"sub_questions_0\"])\n",
    "        new_item = {}\n",
    "        new_item[\"claim_0\"] = item[\"claim_0\"]\n",
    "        new_item[\"sub_questions_0\"] = item[\"sub_questions_0\"]\n",
    "        new_item[f\"sub_question_retrieval_{0}\"] = []\n",
    "        new_data[hop_count][\"SUPPORTED\"].append(new_item)\n",
    "print(total)\n",
    "print(subquestions)\n",
    "print(subquestions/total)\n",
    "\n",
    "save_obj(new_data, \"data/baleen_comparison.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "139.93842709207948"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1000 / (2.382 * 3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.146000000000001"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(2.382 * 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['uid', 'supporting_facts', 'label', 'num_hops', 'hpqa_id', 'previous_iteration_sentences', 'claim_0', 'retrieved_0', 'sub_questions_0', 'sub_question_retrieval_0', 'sub_question_top_sentences_0', 'sub_question_sentences_0', 'top_sentences_0', 'claim_1', 'retrieved_1', 'sub_questions_1', 'sub_question_retrieval_1', 'sub_question_sentences_1', 'top_sentences_1', 'claim_2', 'retrieved_2'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['uid', 'supporting_facts', 'label', 'num_hops', 'hpqa_id', 'previous_iteration_sentences', 'claim_0', 'retrieved_0', 'sub_questions_0', 'sub_question_retrieval_0', 'top_sentences_0', 'claim_1', 'retrieved_1', 'sub_questions_1', 'sub_question_retrieval_1', 'top_sentences_1', 'claim_2', 'retrieved_2', 'tfidf_retrieved_0', 'tfidf_retrieved_1', 'tfidf_retrieved_2', 'decomposed_claims_0', 'decomposed_claims_retrieval_0', 'decomposed_claims_1', 'decomposed_claims_retrieval_1', 'decomposed_claims_2', 'decomposed_claims_retrieval_2'])\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "data = load_obj(\"data/iterative_decomp_FULL_DATASET2.json\")\n",
    "pprint.pprint(data[\"2\"][\"SUPPORTED\"][0].keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data/qualitative_analysis_claims_10_base.json\n",
    "# Alle folgenden experimente basieren auf data/qualitative_analysis_claims_10_base.json\n",
    "\n",
    "# data/iterative_qualitative_analysis_question_answering_60.json\n",
    "# data/iterative_test_base_60_no_filter.json\n",
    "# data/iterative_test_base_60.json\n",
    "# data/iterative_test2.json\n",
    "# data/iterative_test_with_questions.json -> double cross\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_obj\n",
    "qa_data = load_obj(\"data/qualitative_analysis_claims_10_base.json\")\n",
    "setting_data = load_obj(\"data/iterative_test_with_questions.json\")\n",
    "for hop_count in qa_data:\n",
    "    for key in qa_data[hop_count]:\n",
    "        for index, item in enumerate(qa_data[hop_count][key]):\n",
    "            if item[\"claim\"] != setting_data[hop_count][key][index][\"claim\"]:\n",
    "                print(\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:green;\">Not Supported analysis</h1>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success rate for hop 2 iteration 0 SUPPORTED is 70.0%\n",
      "Success rate for hop 2 iteration 0 NOT_SUPPORTED is 70.0%\n",
      "Success rate for hop 2 iteration 1 SUPPORTED is 70.0%\n",
      "Success rate for hop 2 iteration 1 NOT_SUPPORTED is 70.0%\n",
      "Success rate for hop 2 iteration 2 SUPPORTED is 60.0%\n",
      "Success rate for hop 2 iteration 2 NOT_SUPPORTED is 60.0%\n",
      "Success rate for hop 3 iteration 0 SUPPORTED is 40.0%\n",
      "Success rate for hop 3 iteration 0 NOT_SUPPORTED is 40.0%\n",
      "Success rate for hop 3 iteration 1 SUPPORTED is 40.0%\n",
      "Success rate for hop 3 iteration 1 NOT_SUPPORTED is 30.0%\n",
      "Success rate for hop 3 iteration 2 SUPPORTED is 50.0%\n",
      "Success rate for hop 3 iteration 2 NOT_SUPPORTED is 30.0%\n",
      "Success rate for hop 3 iteration 3 SUPPORTED is 60.0%\n",
      "Success rate for hop 3 iteration 3 NOT_SUPPORTED is 30.0%\n",
      "Success rate for hop 4 iteration 0 SUPPORTED is 40.0%\n",
      "Success rate for hop 4 iteration 0 NOT_SUPPORTED is 40.0%\n",
      "Success rate for hop 4 iteration 1 SUPPORTED is 60.0%\n",
      "Success rate for hop 4 iteration 1 NOT_SUPPORTED is 40.0%\n",
      "Success rate for hop 4 iteration 2 SUPPORTED is 60.0%\n",
      "Success rate for hop 4 iteration 2 NOT_SUPPORTED is 30.0%\n",
      "Success rate for hop 4 iteration 3 SUPPORTED is 30.0%\n",
      "Success rate for hop 4 iteration 3 NOT_SUPPORTED is 30.0%\n",
      "Success rate for hop 4 iteration 4 SUPPORTED is 30.0%\n",
      "Success rate for hop 4 iteration 4 NOT_SUPPORTED is 30.0%\n"
     ]
    }
   ],
   "source": [
    "from evaluate_retrieval import is_successful_retrieval\n",
    "from utils import load_obj\n",
    "\n",
    "data = load_obj(\"data/iterative_qualitative_analysis_not_supported_question_answering_80.json\")\n",
    "\n",
    "\n",
    "\n",
    "def calculate_iteration_success(data, hop_count):\n",
    "    \n",
    "    \n",
    "    for i in range(int(hop_count)+1):\n",
    "        success = 0\n",
    "        success_not_supported = 0\n",
    "        total = 0\n",
    "        for item in data:\n",
    "            total += 1\n",
    "            if is_successful_retrieval(item, f\"retrieved_{i}\"):\n",
    "                success += 1\n",
    "            if is_successful_retrieval(item[\"not_supported_counterpart\"], f\"retrieved_{i}\"):\n",
    "                success_not_supported += 1\n",
    "            \n",
    "        print(f\"Success rate for hop {hop_count} iteration {i} SUPPORTED is {success/total * 100}%\")\n",
    "        print(f\"Success rate for hop {hop_count} iteration {i} NOT_SUPPORTED is {success_not_supported/total * 100}%\")\n",
    "\n",
    "for hop_count in data:\n",
    "    calculate_iteration_success(data[hop_count], hop_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_latex_table(data, max_hop_count, max_iter):\n",
    "    for hop_count in range(1, max_hop_count + 1):\n",
    "        hop_count = int(hop_count)\n",
    "        # Print supported and not supported sections for each hop count\n",
    "        print(f\"\\\\multicolumn{{6}}{{l}}{{\\\\textit{{Hop {hop_count}, Supported}}}} \\\\\\\\\")\n",
    "        for config in ['Decomposed', 'Base like Decomposed', 'Base 60 like Decomposed', 'Base 60 no filter like Decomposed', 'Questions Double Cross like Decomposed', 'Questions 60 like Decomposed', 'Questions 60 no filter like Decomposed']:\n",
    "            row_entries = []\n",
    "            for i in range(max_iter + 1):\n",
    "                if i <= hop_count:\n",
    "                    success = sum(is_successful_retrieval(item, f\"retrieved_{i}\") for item in data if item['config'] == config and item['supported'])\n",
    "                    total = sum(1 for item in data if item['config'] == config and item['supported'])\n",
    "                    success_rate = success / total * 100 if total > 0 else 0\n",
    "                    row_entries.append(f\"{success_rate:.1f}%\")\n",
    "                else:\n",
    "                    row_entries.append(\"-\")\n",
    "            print(config + \" & \" + \" & \".join(row_entries) + \" \\\\\\\\\")\n",
    "        \n",
    "        print(f\"\\\\multicolumn{{6}}{{l}}{{\\\\textit{{Hop {hop_count}, Not Supported}}}} \\\\\\\\\")\n",
    "        for config in ['Decomposed', 'Base like Decomposed', 'Base 60 like Decomposed', 'Base 60 no filter like Decomposed', 'Questions Double Cross like Decomposed', 'Questions 60 like Decomposed', 'Questions 60 no filter like Decomposed']:\n",
    "            row_entries = []\n",
    "            for i in range(max_iter + 1):\n",
    "                if i <= hop_count:\n",
    "                    success = sum(is_successful_retrieval(item[\"not_supported_counterpart\"], f\"retrieved_{i}\") for item in data if item['config'] == config and not item['supported'])\n",
    "                    total = sum(1 for item in data if item['config'] == config and not item['supported'])\n",
    "                    success_rate = success / total * 100 if total > 0 else 0\n",
    "                    row_entries.append(f\"{success_rate:.1f}%\")\n",
    "                else:\n",
    "                    row_entries.append(\"-\")\n",
    "            print(config + \" & \" + \" & \".join(row_entries) + \" \\\\\\\\\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_latex_row_for_method(data, max_iter, method, retrieval_key):\n",
    "    # Print rows for each hop count for the specific method\n",
    "    for hop_count in data:\n",
    "        #hop_count = int(hop_count)\n",
    "        # Supported\n",
    "        row_entries_supported = []\n",
    "        print(f\"\\\\multicolumn{{6}}{{l}}{{\\\\textit{{Hop {hop_count}, Supported}}}} \\\\\\\\\")\n",
    "        for i in range(max_iter + 1):\n",
    "            if i <= int(hop_count):\n",
    "                success = sum(is_successful_retrieval(item, f\"{retrieval_key}_{i}\") for item in data[hop_count])\n",
    "                total = sum(1 for item in data[hop_count])\n",
    "                success_rate = success / total * 100 if total > 0 else 0\n",
    "                row_entries_supported.append(f\"{success_rate:.1f}\\%\")\n",
    "            else:\n",
    "                row_entries_supported.append(\"-\")\n",
    "        print(method + \" & \" + \" & \".join(row_entries_supported) + \" \\\\\\\\\")\n",
    "\n",
    "        # Not Supported\n",
    "        row_entries_not_supported = []\n",
    "        print(f\"\\\\multicolumn{{6}}{{l}}{{\\\\textit{{Hop {hop_count}, Not Supported}}}} \\\\\\\\\")\n",
    "        for i in range(max_iter + 1):\n",
    "            if i <= int(hop_count):\n",
    "                success = sum(is_successful_retrieval(item[\"not_supported_counterpart\"], f\"{retrieval_key}_{i}\") for item in data[hop_count])\n",
    "                total = sum(1 for item in data[hop_count])\n",
    "                success_rate = success / total * 100 if total > 0 else 0\n",
    "                row_entries_not_supported.append(f\"{success_rate:.1f}\\%\")\n",
    "            else:\n",
    "                row_entries_not_supported.append(\"-\")\n",
    "        print(method + \" & \" + \" & \".join(row_entries_not_supported) + \" \\\\\\\\\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "textbf{Iter 0} & textbf{Iter 1} & textbf{Iter 2} & textbf{Iter 3} & textbf{Iter 4}\n",
      "tfidf & 39.23\\% & 43.35\\% & 41.08\\% & 39.67\\% & 39.42\\%\\\\\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['uid', 'supporting_facts', 'label', 'num_hops', 'hpqa_id', 'previous_iteration_sentences', 'claim_0', 'retrieved_0', 'sub_questions_0', 'sub_question_retrieval_0', 'sub_question_sentences_0', 'top_sentences_0', 'claim_1', 'retrieved_1', 'sub_questions_1', 'sub_question_retrieval_1', 'sub_question_sentences_1', 'top_sentences_1', 'claim_2', 'retrieved_2', 'sub_questions_2', 'sub_question_retrieval_2', 'sub_question_sentences_2', 'top_sentences_2', 'claim_3', 'sub_questions_3', 'sub_question_retrieval_3', 'sub_question_sentences_3', 'top_sentences_3', 'claim_4', 'retrieved_3', 'retrieved_4'])\n"
     ]
    }
   ],
   "source": [
    "print(data[\"4\"][\"SUPPORTED\"][10].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_obj\n",
    "qa_data = load_obj(\"data/qualitative_analysis_claims_10_base.json\")\n",
    "setting_data = load_obj(\"data/iterative_test_with_questions.json\")\n",
    "for hop_count in qa_data:\n",
    "    for index, item in enumerate(qa_data[hop_count]):\n",
    "        if item[\"claim\"] != setting_data[hop_count][index][\"claim\"]:\n",
    "            print(\"ERROR\")\n",
    "\n",
    "# not supported style data\n",
    "# rohdaten: data/qualitative_analysis_not_supported.json\n",
    "\n",
    "# iterative_qualitative_analysis_not_supported_no_filter_80.json\n",
    "# iterative_qualitative_analysis_not_supported_question_answering_80.json\n",
    "# iterative_qualitative_analysis_not_supported_no_filter_80.json -> normales sub question retrieval\n",
    "\n",
    "# iterative_not_supported_subquestions_filter_60.json\n",
    "# iterative_not_supported_base_no_filter_60_100_docs_retrieved.json -> l채uft screen 2 DONE\n",
    "# data/iterative_not_supported_subquestions_no_filter_60.json -> DONE\n",
    "# data/iterative_not_supported_subquestions_no_filter_change_prompt_60.json -> DONE\n",
    "# data/iterative_not_supported_question_answering_60.json -> l채uft\n",
    "# data/iterative_not_supported_base_no_filter_60_based_on_subquestions.json \n",
    "\n",
    "# \"data/iterative_qualitative_analysis_question_answering_60.json\" screen 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'generate_latex_row_for_method' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_obj\n\u001b[1;32m      2\u001b[0m data\u001b[38;5;241m=\u001b[39m load_obj(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/iterative_decomp_FULL_DATASET_1_combined.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mgenerate_latex_row_for_method\u001b[49m(data\u001b[38;5;241m=\u001b[39mdata, max_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDecomp 1\u001b[39m\u001b[38;5;124m\"\u001b[39m, retrieval_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecomposed_combined\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'generate_latex_row_for_method' is not defined"
     ]
    }
   ],
   "source": [
    "from utils import load_obj\n",
    "data= load_obj(\"data/iterative_not_supported_subquestions_no_filter_change_prompt_60.json\")\n",
    "generate_latex_row_for_method(data=data, max_iter=4, method=\"Subquestions Entity Correction\", retrieval_key=\"retrieved\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mistral_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
